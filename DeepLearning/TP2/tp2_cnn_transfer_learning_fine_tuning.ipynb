{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2: Seafloor classification par CNN \n",
    "\n",
    "## 1 - Introduction\n",
    "\n",
    "Disposant d'un ensemble d'images dont on veut prédire la classe, deux possibilités s'offrent pour apprendre un modèle profond de classement.\n",
    "\n",
    "- La première possibilité se nomme \"Transfer Learning\" associé au \"fine tuning\" dont les principes sont d'utiliser un réseau de neurones profond entrainé dans un autre contexte et de l'adapter à nos données: \n",
    "    \n",
    "- La seconde possibilité est de créer et d'entrainer un modèle profond ex-nihilo (from scratch, en partant de zéro).\n",
    "\n",
    "\n",
    "L'objectif de ce TP est d'appliquer ces deux possibilités au problème de classification de patchs d'images sonar en types de fond marin que vous avez déjà traités dans les TPs précédent. Vous reprendrez les fonctions d'import des patchs que vous avez déjà mises au point lors des tps précédents.\n",
    "\n",
    "Il pourra être néanmoins utile d'utiliser les lignes de code suivantes pour que les données soient dans la forme attendue par tensoflow/keras et éventuellement changer la taille des images:\n",
    "```python\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "target_size = 192;\n",
    "feature_values = np.array([img_to_array(load_img(img,  # color_mode = \"grayscale\",\n",
    "             target_size=(target_size, target_size))) for img in dataset_df['image_path'].values.tolist()\n",
    "]).astype('float32')\n",
    "```\n",
    "A noter que la taille des patchs est automatiquement réduite à 192x192 pixels, vous pouvez réduire cette taille en fonction des performances de votre machine. Attention, une taille trop basse peut impliquer une erreur si vous réutilisez par la suite un modèle déjà entrainé (par exemple VGG16).\n",
    "\n",
    "La création d'ensemble d'apprentissage, de validation et de test se fera en divisant la base en trois parts. Il faudrait pour ce petit jeu de données réaliser une procédure de cross-validation. Compte tenu du temps pour réaliser cette procédure, elle sera ici laissée de coté. \n",
    "\n",
    "## 1 - Tutorials CNN et transfer learning par fine tuning\n",
    "\n",
    "Dans le startercode, vous trouverez un jupyter notebooks qui vous servira de base pour réaliser la suite. Dans un premier temps, le notebook détaille le fonctionnement et la mise en oeuvre des CNNs (en particulier les différentes couches d'un CNN avec des exemples); pour ensuite détailler la procédure liée au fine tuning avec data augmentation à partir de données de type TensorFlow Dataset. \n",
    "\n",
    "Veillez à bien suivre les différentes étapes et à bien comprendre les différentes commandes employées. Si vous voulez d'autres exemples, vous trouverez d'autres ressources supplémentaires.\n",
    "\n",
    "\n",
    "## 2 - Transfer learning par fine tuning sur le dataset seafloor\n",
    "\n",
    "- Vous commencerez par le fine tuning en vous inspirant du tuto fourni ci-dessus pour faire du transfer learning du modèle xception (dont les paramètres ont été appris sur la base d'images \"imageNet\") pour l'appliquer aux patchs d'images sonar. Vous procéderez ainsi:\n",
    "  - les modèles sont téléchargeables ici si vous avez des problèmes pour télécharger: https://drive.google.com/open?id=1qFwqoNU1fsvl8fu-7eRCmjoPNZZNzPSJ\n",
    "  - Résumer l'approche du transfer learning/fine tuning\n",
    "  - Décriver l'architecture du modèle utilisé (xception ici)\n",
    "  - Vous précisérez votre choix concernant les paramètres des fonctions appelées en particulier expliquer votre démarche concernant les phases de preprocessing des images, de data augmentation, de classification, etc.\n",
    "  - remarque: comme les images sonar sont en niveaux de gris et que le modèle VGG prend en entrée des images couleurs, il s'agira de dupliquer ce canal sur les canaux R, G et B. \n",
    "\n",
    "  - Enfin, vous évaluerez les performances obtenues.\n",
    "- (Bonus) comparez les résultats obtenus par l'architecture vgg16 (https://keras.io/applications/).\n",
    "\n",
    "- (Bonus) Essayez et comparez les résultats obtenus par d'autres architectures (Resnet, Inception etc...https://keras.io/applications/).\n",
    "\n",
    "## 3 - Proposition de votre propre achitecture  \n",
    "\n",
    "- Vous proposerez ensuite une architecture de réseau profond convolutif et évaluerez ses performances. \n",
    "- Expliquez brièvement votre architecture et en particulier à quoi servent les couches (et leur enchainement) de votre architecture.\n",
    "- Vous comparerez ensuite les performances obtenus (par rapport à ceux obtenus à la partie précédente) sur la matrice de confusion et les métriques de performance classiques.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A rendre \n",
    "- pour le **12/01/21**\n",
    "- la séance du 05/01/21 sera consacrée à finaliser\n",
    "- **Commenter au maximum votre code (pourquoi vous utilisez tel ou tel bout de code) ou apporter des précisions dans votre CR.**\n",
    "- au choix (**N'oublier pas les deux noms en cas de binômes**):\n",
    "    - un fichier zip avec *.py et un cr au format pdf\n",
    "    - un fichier .ipynb avec compte-rendu et code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Aide pour démarrer\n",
    "\n",
    "## 4.1 Chargement des données\n",
    "Vous pourrez utiliser cette procédure pour charger les données: \n",
    "**A noter**\n",
    "- target_size permet de définir un éventuel changement de taille des images qui pourra servir en fonction de la taille d'entrée du modèle que vous considérez.\n",
    "- comme les images sonar sont en niveaux de gris et que le modèle VGG prend en entrée des images couleurs, load_img duplique ce canal sur les canaux R, G et B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Paramètres\n",
    "if IN_COLAB:\n",
    "    DATASET_PATH = 'gdrive/My Drive/Colab Notebooks/ex06_supervised_seabedClassification/dataset/imgs/'\n",
    "    LABEL_PATH = 'gdrive/My Drive/Colab Notebooks/ex06_supervised_seabedClassification/dataset/labels/labels.csv'\n",
    "else:\n",
    "    IN_COLAB = False\n",
    "    DATASET_PATH = r'./dataset/imgs/'\n",
    "    LABEL_PATH = r'./dataset/labels/labels.csv'\n",
    "\n",
    "    \n",
    "# \n",
    "target_size = 200\n",
    "    \n",
    "    \n",
    "# Charger le fichier CSV\n",
    "\n",
    "dataset_df = pd.read_csv(LABEL_PATH)\n",
    "\n",
    "\n",
    "# We add another column to the labels dataset to identify image path\n",
    "dataset_df['image_path'] = dataset_df.apply(lambda row: (DATASET_PATH + row[\"id\"]), axis=1)\n",
    "\n",
    "batch_imgs = np.array([img_to_array(\n",
    "    load_img(img,  # color_mode = \"grayscale\",\n",
    "             target_size=(target_size, target_size))\n",
    ") for img\n",
    "    in dataset_df['image_path'].values.tolist()\n",
    "]).astype('float32')\n",
    "\n",
    "\n",
    "#  Noms des labels dans l'ordre, respectivement aux indices\n",
    "labelNames_unique = np.array([\"Posidonia\",\"Ripple 45°\",\"Ripple vertical\",\"Rock\",\"Sand\",\"Silt\"])\n",
    "labelDict={}\n",
    "for i in range(len(labelNames_unique)):\n",
    "    labelDict.update({i:labelNames_unique[i]})\n",
    "\n",
    "    \n",
    "# Récupération des labels\n",
    "label_names = dataset_df['seafloor']\n",
    "\n",
    "\n",
    "# nb de classes\n",
    "label_nb = labelNames_unique.shape[0]\n",
    "\n",
    "# indices\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labelNames_unique)\n",
    "labelIndices_unique = le.transform(labelNames_unique)\n",
    "labelIndices  = le.transform(label_names)\n",
    "\n",
    "\n",
    "# one-hot-encoding\n",
    "labelOhe = pd.get_dummies(label_names.reset_index(drop=True)).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Définition, entrainer et évaleur le modèle VGG16 (par exemple) pour le fine tuning\n",
    "Ensuite procéder comme le tuto:\n",
    "- charger le modèle VGG16 sans le classifieur include_top=False\n",
    "- Visualiser l'architecture du modèle: model.summary()\n",
    "- Créer un modèle d'extraction d'information (features) allant de la couche d'entrée de VGG16 jusqu'à sa dernière couche de convolution nommée 'block3_pool'\n",
    "- Rajouter des couches Dense pour définir un classifieur Fully connected (attention aux nombres de sorties de la dernière couche et à sa fonction d'activation)\n",
    "- ne pas oublier l'étape de preprocessing essentiel à la bonne réussite de l'apprentissage. Il faut se renseigner sur les corrections à apporter et la taille des images d'entrée\n",
    "- Compiler (compile), entrainer (fit) et évaluer (evaluate) le modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Fonctions d'aide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c57243e1f279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# fonction callback (à rajouter en option à model.fit) pour suivre l'évolution de la matrice de confusion au long de l'apprentissage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# credit: K. Bedin (ROB 2020) développé lors du cours\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mConfusionEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \"\"\"\n\u001b[1;32m     31\u001b[0m         \u001b[0mFonction\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0mpour\u001b[0m \u001b[0mla\u001b[0m \u001b[0mméthode\u001b[0m \u001b[0;34m'fit_generator()'\u001b[0m \u001b[0mpermettant\u001b[0m \u001b[0md\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mafficher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Callback' is not defined"
     ]
    }
   ],
   "source": [
    "# fonction d'aide pour afficher les courbes d'apprentissage\n",
    "\n",
    "def learningCurves(history,title):\n",
    "    #Learning curve plotting\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    t = f.suptitle(title, fontsize=12)\n",
    "    f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "    epochs = list(range(1,NB_EPOCHS+1))\n",
    "    ax1.plot(epochs, history['accuracy'], label='Train Accuracy')\n",
    "    ax1.plot(epochs, history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_xticks(epochs)\n",
    "    ax1.set_ylabel('Accuracy Value')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_title('Accuracy')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(epochs, history['loss'], label='Train Loss')\n",
    "    ax2.plot(epochs, history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_xticks(epochs)\n",
    "    ax2.set_ylabel('Loss Value')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_title('Loss')\n",
    "    ax2.legend()\n",
    "\n",
    "    \n",
    "# fonction callback (à rajouter en option à model.fit) pour suivre l'évolution de la matrice de confusion au long de l'apprentissage\n",
    "# credit: K. Bedin (ROB 2020) développé lors du cours\n",
    "class ConfusionEvaluation(Callback):\n",
    "    \"\"\"\n",
    "        Fonction callback pour la méthode 'fit_generator()' permettant d'afficher \n",
    "        la matrice de confusion à chaque fin d'Epoch.\n",
    "        Cela permet de visualiser concraitement l'évolution de la classification au cours de l'apprentissage.\n",
    "    \"\"\"\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        preds_Inception = self.model.predict(self.X_val/255.)\n",
    "        matrixInception = confusion_matrix(self.y_val,preds_Inception.argmax(axis=1))\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(matrixInception)\n",
    "\n",
    "#cbk_matconf = ConfusionEvaluation(validation_data=(featValues_val, labelInd_val))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 -  Ressources supplémentaires en transfer learning par fine tuning**\n",
    " En dehors des supports de cours, vous pourrez aussi vous appuyer sur:\n",
    "- Les concepts du transfer learning sont expliqués dans les liens ci-dessous:\n",
    "  - https://www.youtube.com/watch?v=FQM13HkEfBk&index=20&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF\n",
    "  - http://cs231n.github.io/transfer-learning/\n",
    "  - https://flyyufelix.github.io/2016/10/03/fine-tuning-in-keras-part1.html et https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html\n",
    "- Des exemples supplémentaires d'implémentation\n",
    "  - https://github.com/dipanjanS/hands-on-transfer-learning-with-python/blob/master/notebooks/Ch06%20-%20Image%20Recognition%20and%20Classification/CIFAR10_CNN_Classifier.ipynb et https://github.com/dipanjanS/hands-on-transfer-learning-with-python/blob/master/notebooks/Ch06%20-%20Image%20Recognition%20and%20Classification/CIFAR10_VGG16_Transfer_Learning_Classifier.ipynb\n",
    "  - https://github.com/dipanjanS/hands-on-transfer-learning-with-python/blob/master/notebooks/Ch06%20-%20Image%20Recognition%20and%20Classification/Dog_Breed_EDA.ipynb et https://github.com/dipanjanS/hands-on-transfer-learning-with-python/blob/master/notebooks/Ch06%20-%20Image%20Recognition%20and%20Classification/Dog_Breed_Transfer_Learning_Classifier.ipynb\n",
    "- cours et des vidéos de Stanford University: https://www.youtube.com/watch?v=wEoyxE0GP2M&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=6, https://www.youtube.com/watch?v=wEoyxE0GP2M&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=7)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
