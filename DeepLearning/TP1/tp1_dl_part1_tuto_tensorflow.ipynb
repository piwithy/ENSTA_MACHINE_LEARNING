{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JÉZÉGOU Pierre-Yves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1: exploration de la librairie Tensorflow 2.x\n",
    "\n",
    "Jusqu'à présent, vous avez toujours utilisé numpy pour construire des petits réseaux de neurones. Nous allons maintenant découvrir un framework plus adapté pour l'apprentissage et l'utilisation de modèles de réseaux de neurones possédants un grand nombre de paramètres. Ce type de modèles fait usage de nombreux calculs qui peuvent être parallélisés. Un framework de machine learning (tel que TensorFlow, pyTorch, etc.) faisant usage de calculs sur GPUs peut alors accélérer considérablement le développement de votre algorithme d'apprentissage. Ces frameworks permettent également (comme scikit-learn) la création rapide et simplifiée de réseaux de neurones.\n",
    "Tous ces frameworks sont accompagnés d'une abondante documentation, que vous devriez pouvoir lire librement. \n",
    "\n",
    "Dans ce TP, vous apprendrez avec TensorFlow 2.0 à initialiser les tenseurs et calculer mais aussi à élaborer, apprendre, analyser et évaluer un modèle de réseau de neurones.\n",
    "\n",
    "Vous devez:\n",
    "- compléter le code aux endroits indiqués;\n",
    "- élaborer, apprendre, analyser et évaluer avec tensorflow, un réseau de neurones sur la base d'images de fonds marins (rencontrés dans un TP précédent)\n",
    "\n",
    "Ce tutoriel est largement basé sur ces sources: \n",
    " - Francois Chollet's github (Tensorflow_2_0_+_Keras_Crash_Course.ipynb)\n",
    " - Boscher's github (v02_Tensorflow_2_0_+_Keras_Crash_Course.ipynb)\n",
    " - MIT course's github (Part1_TensorFlow.ipynb)\n",
    " - Magnus Erik Hvass Pedersen's github (https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/01_Simple_Linear_Model.ipynb)\n",
    "\n",
    "Le framework TensorFlow tire son nom de sa gestion des flux (flow = node = opération mathématique) de tenseurs, qui sont des structures de données que l'on peut considérer comme des tableaux multidimensionnels. Les tenseurs sont représentés par des tableaux à $n$ dimensions de types de données de base tels qu'une chaîne ou un entier. Ils permettent de généraliser les vecteurs et les matrices à des dimensions plus élevées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Imports\n",
    "\n",
    "Pour pouvoir commencer, vous importerez les librairies suivantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Déclarer des tenseurs constants\n",
    "\n",
    "Vous vous reporterez à [l'adresse suivante](https://www.tensorflow.org/guide/tensor) pour plus de détails.\n",
    "\n",
    "\n",
    "### 2.1 - Tenseurs 0-d\n",
    "Examinons d'abord les tenseurs 0-d, dont un scalaire, une chaîne de caractères sont des exemples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Tennis', shape=(), dtype=string)\n",
      "tf.Tensor(1.41421356237, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "sport = tf.constant(\"Tennis\", tf.string)\n",
    "number = tf.constant(1.41421356237, tf.float64)\n",
    "print(sport)\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - tenseurs constants 1-d\n",
    "A common way to create constant tensors is via `tf.ones` and `tf.zeros` (just like `np.ones` and `np.zeros`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.ones(shape=(2, 1)))\n",
    "print(tf.zeros(shape=(2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors and lists can be used to create 1-d Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'Tennis' b'Basketball'], shape=(2,), dtype=string)\n",
      "tf.Tensor([3.141592 1.414213 2.71821 ], shape=(3,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "sports = tf.constant([\"Tennis\", \"Basketball\"], tf.string)\n",
    "numbers = tf.constant([3.141592, 1.414213, 2.71821], tf.float64)\n",
    "print(sports)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - tenseurs n-d\n",
    "Ensuite, nous pouvons créer des tenseurs 2-d (c'est-à-dire des matrices) et des tenseurs de rang supérieur. Dans les TP suivants de traitement d'images, nous utiliserons des tenseurs 4-d. Ici, les dimensions correspondent au nombre d'images d'exemple dans notre base de données, à la hauteur et à la largeur de l'image, ainsi qu'au nombre de canaux de couleur.\n",
    "                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "### Defining higher-order Tensors ###\n",
    "\n",
    "''' START CODE HERE '''\n",
    "''' Define a 2-d Tensor.'''\n",
    "matrix = tf.zeros(shape=(3,3))# TODO\n",
    "''' END CODE HERE ''' \n",
    "\n",
    "print(matrix)\n",
    "\n",
    "# les méthodes suivantes seront expliquées plus tard\n",
    "assert isinstance(matrix, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
    "assert tf.rank(matrix).numpy() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]], shape=(10, 256, 256, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "''' START CODE HERE '''\n",
    "''' Define a 4-d Tensor.'''\n",
    "# Use tf.zeros to initialize a 4-d Tensor of zeros with size 10 x 256 x 256 x 3. \n",
    "#   You can think of this as 10 images where each image is RGB 256 x 256.\n",
    "images =  tf.zeros(shape=(10,256,256,3))# TODO\n",
    "''' END CODE HERE ''' \n",
    "\n",
    "\n",
    "print(images)\n",
    "\n",
    "# les méthodes suivantes seront expliquées plus tard\n",
    "assert isinstance(images, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
    "assert tf.rank(images).numpy() == 4, \"matrix must be of rank 4\"\n",
    "assert tf.shape(images).numpy().tolist() == [10, 256, 256, 3], \"matrix is incorrect shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Générer des nombres aléatoires\n",
    "- créer un tenseur constant contenant des réels générés aléatoirement par une **distribution gaussienne** dans : `tf.random.normal`\n",
    "- créer un tenseur constant contenant des entiers générés aléatoirement par une **distribution uniforme** dans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-1.1012203 ,  1.5457517 ],\n",
       "       [ 0.383644  , -0.87965786]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal(shape=(2, 2), mean=0., stddev=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[2, 0],\n",
       "       [2, 1]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Trouver/changer les caractéristiques des tenseurs (méthodes de tenseurs)\n",
    "\n",
    "#### 3.1 - Convertir de tensorflow vers numpy (et inversement)\n",
    "\n",
    "Vous pouvez faire la conversion par un appel à la fonction `.numpy()` and faire la conversion inverse avec  `tf.convert_to_tensor()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n",
      "vers numpy:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "modif avec numpy: \n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "retour à tf:\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "''' START CODE HERE '''\n",
    "''' choose a previously defined tensor.'''\n",
    "T =  matrix\n",
    "''' END CODE HERE ''' \n",
    "\n",
    "print(T)\n",
    "\n",
    "print('vers numpy:')\n",
    "T_np=T.numpy()\n",
    "print(T_np)\n",
    "\n",
    "print(\"modif avec numpy: \")\n",
    "T_np += np.array([1,2,3,4,5,6,7,8,9]).reshape(3,3)\n",
    "print(T_np)\n",
    "\n",
    "print('retour à tf:')\n",
    "tf_T = tf.convert_to_tensor(T_np)\n",
    "print(tf_T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - Gérer le data type: connaitre, convertir\n",
    "\n",
    "- Choisissez un des tenseurs définis précédemment ou initialiser en un nouveau;\n",
    "- La commande `dtype` permet de s'informer sur son data type (Plus de détails à l'[adresse suivante](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType));\n",
    "- Réalisez les conversions de type avec `tf.cast`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`T` is a Tensor with data type: <dtype: 'float32'>\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]], shape=(3, 3), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "''' START CODE HERE '''\n",
    "''' choose a previously defined tensor.'''\n",
    "T =  tf_T\n",
    "''' END CODE HERE ''' \n",
    "\n",
    "print(\"`T` is a Tensor with data type: {}\".format(T.dtype))\n",
    "\n",
    "# cast to float\n",
    "T_f32 = tf.cast(T, dtype=tf.float32)\n",
    "print(T_f32)\n",
    "\n",
    "print(tf.cast(T, dtype=tf.int64))\n",
    "\n",
    "#print(tf.cast(T, dtype=tf.string))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - Connaitre le nombre de dimensions (rang) et la taille de chaque dimension\n",
    "- La ```shape``` d'un tenseur définit son nombre de dimensions et la taille de chaque dimension. \n",
    "- Le ```rank``` d'un tenseur fournit le nombre de dimensions ($n$-dimensions) -- vous pouvez aussi considérer cela comme l'ordre ou le degré du tenseur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`T` is a 2-d Tensor with shape: [3 3]\n"
     ]
    }
   ],
   "source": [
    "''' START CODE HERE '''\n",
    "''' choose a previously defined tensor.'''\n",
    "T = T\n",
    "''' END CODE HERE ''' \n",
    "\n",
    "print(\"`T` is a {}-d Tensor with shape: {}\".format(tf.rank(T).numpy(), tf.shape(T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 - Sur quel device est stocké le tenseur?\n",
    "La commande `.device` permet de connaitre si le tenseur est stocké dans la mémoire du GPU ou du CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`T` is a /job:localhost/replica:0/task:0/device:CPU:0 Tensor\n"
     ]
    }
   ],
   "source": [
    "''' START CODE HERE '''\n",
    "''' choose a previously defined tensor.'''\n",
    "T = T\n",
    "''' END CODE HERE ''' \n",
    "\n",
    "print(\"`T` is a {} Tensor\".format(T.device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 - Indexer les tenseurs pour accèder aux sous-tenseurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dans numpy, vous pouvez utiliser le découpage pour accéder aux sous-tenseurs d'un tenseur de rang supérieur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`row_vector`: [4. 5. 6.]\n",
      "`column_vector`: [2. 5. 8.]\n",
      "`scalar`: 5.0\n"
     ]
    }
   ],
   "source": [
    "''' START CODE HERE '''\n",
    "''' choose a previously defined tensor.'''\n",
    "T = T \n",
    "''' END CODE HERE ''' \n",
    "\n",
    "assert isinstance(T, tf.Tensor), \"T must be a tf Tensor object\"\n",
    "assert tf.rank(T).numpy() > 0, \"T must be of rank greater to 0\"\n",
    "\n",
    "\n",
    "row_vector = T[1]\n",
    "column_vector = T[:,1]\n",
    "scalar = T[1, 1]\n",
    "\n",
    "print(\"`row_vector`: {}\".format(row_vector.numpy()))\n",
    "print(\"`column_vector`: {}\".format(column_vector.numpy()))\n",
    "print(\"`scalar`: {}\".format(scalar.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Tenseurs variables\n",
    "\n",
    "Les tenseurs [Variables](https://www.tensorflow.org/guide/variable) sont des cas particuliers des tenseurs permettant de stocker en mémoire des états variables (les valeurs des variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[-0.45701224, -0.40686727],\n",
      "       [ 0.72857773, -0.8929778 ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "initial_value = tf.random.normal(shape=(2, 2))\n",
    "a = tf.Variable(initial_value)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour changer la valeurs d'une variable, il faut utiliser les méthodes `.assign(value)`, ou `.assign_add(increment)` ou `.assign_sub(decrement)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_value = tf.random.normal(shape=(2, 2))\n",
    "a.assign(new_value)\n",
    "# check\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_value = tf.random.normal(shape=(2, 2))\n",
    "a.assign_add(added_value)\n",
    "# check\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j] + added_value[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Calculer avec les tenseurs \n",
    "\n",
    "### 5.1 Tensorflow définit des graphes pour calculer\n",
    "\n",
    "Une façon pratique d'envisager et de visualiser les calculs dans TensorFlow est de les effectuer sous forme de graphiques. Nous pouvons définir ce graphique en termes de tenseurs, qui contiennent des données, et les opérations mathématiques qui agissent sur ces tenseurs dans un certain ordre. Prenons un exemple simple, et définissons ce calcul à l'aide de TensorFlow :\n",
    "\n",
    "<img src=\"imgs/add-graph.png\" style=\"width:600px;height:150px ;\">\n",
    "\n",
    "figure tiré de MIT Deep learning course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(76, shape=(), dtype=int32)\n",
      "tf.Tensor(76, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Create the nodes in the graph, and initialize values\n",
    "a = tf.constant(15)\n",
    "b = tf.constant(61)\n",
    "\n",
    "# Add them!\n",
    "c1 = tf.add(a,b)\n",
    "c2 = a + b # TensorFlow overrides the \"+\" operation so that it is able to act on Tensors\n",
    "print(c1)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour obtenit le tenseur de valeur 76, un graphe de calculs composé d'opérations TensorFlow a été créé et exécuté.\n",
    "\n",
    "Considérons maintenant un exemple un peu plus compliqué :\n",
    "\n",
    "<img src=\"imgs/computation-graph.png\" style=\"width:400px;height:100 ;\">\n",
    "figure tirée de MIT Deep learning course\n",
    "\n",
    "Ici, nous considérons deux entrées, `a`, `b`, et calculons une sortie `e`. Chaque noeud du graphique représente une opération qui prend une entrée, effectue un calcul et transmet sa sortie à un autre noeud.\n",
    "\n",
    "Définissons une fonction simple dans TensorFlow pour construire cette fonction de calcul :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining Tensor computations ###\n",
    "\n",
    "# Construct a simple computation function\n",
    "def func(a,b):\n",
    "    ''' START CODE HERE '''\n",
    "    ''' Define the operation for c, d, e (use tf.math.add, tf.math.subtract, tf.math.multiply).'''\n",
    "    c =  tf.math.add(a,b)\n",
    "    d =  tf.math.subtract(b, 1) # TODO\n",
    "    e =  tf.math.multiply(c,d) # TODO\n",
    "    ''' END CODE HERE ''' \n",
    "\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécutons cette fonction par le biais d'un graphe en assignant des valeurs aux variables `a,b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Consider example values for a,b\n",
    "a, b = 1.5, 2.5\n",
    "# Execute the computation\n",
    "e_out = func(a,b)\n",
    "print(e_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autre exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "c = a + b\n",
    "d = tf.square(c)\n",
    "e = tf.exp(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Fonction linéaire (combinaison linéaire)\n",
    "\n",
    "Calculez l'équation suivante : $h_{W,b}(X) = WX + b$, où $W$ et $X$ sont des matrices aléatoires et $b$ est un vecteur aléatoire. \n",
    "\n",
    "- $X$ et $b$ sont tirés d'une distribution normale aléatoire. \n",
    "- $W$ est de dimension (4, 3), $X$ est (3,1) et $b$ est (4,1). \n",
    "\n",
    "Les fonctions suivantes peuvent vous être utiles : \n",
    "- tf.matmul(..., ...) pour effectuer une multiplication matricielle\n",
    "- tf.add(..., ...) pour faire une addition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = tf.Tensor(\n",
      "[[1.5711212]\n",
      " [1.6371589]\n",
      " [1.416699 ]\n",
      " [1.031102 ]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: linear_function\n",
    "\n",
    "def linear_function():\n",
    "    \"\"\"\n",
    "    Implements a linear function: \n",
    "            Initializes W to be a random tensor of shape (4,3)\n",
    "            Initializes X to be a random tensor of shape (3,1)\n",
    "            Initializes b to be a random tensor of shape (4,1)\n",
    "    Returns: \n",
    "    result -- runs the session for Y = WX + b \n",
    "    \"\"\"\n",
    "    \n",
    "    # sGLC: mis ici pour contrôler le résultat: Expected output\n",
    "    tf.random.set_seed(seed)\n",
    "    # eGLC: mis ici pour contrôler le résultat: Expected output\n",
    " \n",
    "    ''' START CODE HERE (4 lines of code)'''\n",
    "    X = tf.random.uniform(shape=(3,1))\n",
    "    W = tf.random.uniform(shape=(4,3))\n",
    "    b = tf.random.uniform(shape=(4,1))\n",
    "    H = tf.add(tf.matmul(W,X),b)\n",
    "    ''' END CODE HERE ''' \n",
    "    \n",
    "    return H\n",
    "\n",
    "print( \"result = \" + str(linear_function()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Expected Output ***: \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "**result**\n",
    "</td>\n",
    "<td>\n",
    "[[-0.9966546 ]\n",
    " [-2.6214728 ]\n",
    " [ 3.096904  ]\n",
    " [ 0.02088326]]\n",
    "</td>\n",
    "</tr> \n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 - Calculer avec la fonction sigmoïde \n",
    "\n",
    "Vous venez de prorammer une fonction linéaire. Tensorflow offre une variété de fonctions de réseaux de neurones couramment utilisées comme `tf.sigmoid` et `tf.softmax`. Pour cet exercice, nous allons calculer la fonction sigmoïde d'une entrée. \n",
    "\n",
    "Implémentez la fonction sigmoïde ci-dessous. Vous devez utiliser ce qui suit : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- input value, scalar or vector\n",
    "    \n",
    "    Returns: \n",
    "    results -- the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the tensorflow sigmoid function\n",
    "    ''' START CODE HERE'''\n",
    "    result = 1/(1+ tf.math.exp(-x))\n",
    "    ''' END CODE HERE '''\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = tf.Tensor(0.5, shape=(), dtype=float32)\n",
      "sigmoid(12) = tf.Tensor(0.9999938, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid(0) = \" + str(sigmoid(0.)))\n",
    "print (\"sigmoid(12) = \" + str(sigmoid(12.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Expected Output ***: \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "**sigmoid(0)**\n",
    "</td>\n",
    "<td>\n",
    "0.5\n",
    "</td>\n",
    "</tr>\n",
    "<tr> \n",
    "<td>\n",
    "**sigmoid(12)**\n",
    "</td>\n",
    "<td>\n",
    "0.999994\n",
    "</td>\n",
    "</tr> \n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Calcul de la fonction de coût \n",
    "\n",
    "#### 5.4.1 Calcul de fonction de cout de régression \n",
    "\n",
    "Calculez avec Tensorflow la fonction suivante:\n",
    "$$loss = \\mathcal{L}(\\hat{y}, y) = (\\hat y^{(i)} - y^{(i)})^2 \\tag{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "y_hat = tf.constant(36, name='y_hat')            # Define y_hat constant. Set to 36.\n",
    "y = tf.constant(39, name='y')                    # Define y. Set to 39\n",
    "\n",
    "''' START CODE HERE '''\n",
    "''' Define the operation for loss.'''\n",
    "loss =  tf.math.square(tf.math.subtract(y_hat, y)) # Create a variable for the loss\n",
    "''' END CODE HERE '''\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 - Calcul de la fonction de coût \"entropie croisée\" de classification\n",
    "\n",
    "Vous pouvez utiliser une fonction intégrée pour calculer le coût de votre réseau de neurones. Ainsi, au lieu de devoir écrire du code pour le calculer en fonction de $h_\\theta^{(i)}$ et de $y^{(i)}$ pour $i\\in\\{1,\\dots,m\\}$:\n",
    "\n",
    "$$ J(\\theta) = - \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log h_\\theta^{(i)}(x) + (1-y^{(i)})\\log (1-h_\\theta^{(i)}(x) )\\large )\\small\\tag{2}$$, vous pouvez appeler la fonction `tf.nn.sigmoid_cross_entropy_with_logits(logits = ..., labels = ...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: cost\n",
    "\n",
    "def cost(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes the cost using the sigmoid cross entropy\n",
    "    \n",
    "    Arguments:\n",
    "    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n",
    "    labels -- vector of labels y (1 or 0) \n",
    "        \n",
    "    Returns:\n",
    "    cost -- the cost \n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the loss function (approx. 1 line)\n",
    "    ''' START CODE HERE '''\n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits =logits, labels =labels)\n",
    "    ''' END CODE HERE '''\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour évaluer la fonction cost, vous devez mettre en entrée logits et labels. La variable logits sera ici la sortie de la fonction `linear_function()` précédemment définie. Définissre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_i = tf.Tensor(\n",
      "[[-0.70849025]\n",
      " [-0.865376  ]\n",
      " [-0.37320104]\n",
      " [ 0.27292243]], shape=(4, 1), dtype=float32)\n",
      "cost = tf.Tensor(-0.41853622, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "''' START CODE HERE '''\n",
    "''' Define logits as the outputs of linear_function and some compatible labels (use tf.transpose if )'''\n",
    "logits = linear_function()\n",
    "labels =  linear_function()\n",
    "''' END CODE HERE '''\n",
    "\n",
    "# losses and loss\n",
    "cost_i = cost(logits, labels)\n",
    "print (\"cost_i = \" + str(cost_i))\n",
    "\n",
    "cost_tot = tf.math.reduce_mean(cost_i)\n",
    "print (\"cost = \" + str(cost_tot))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Expected Output ***: \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "**cost_i**\n",
    "</td>\n",
    "<td>\n",
    "[[0.3141625 ]\n",
    " [0.07017484]\n",
    " [0.04419762]\n",
    " [0.68276006]]\n",
    "</td>\n",
    "</tr>\n",
    "<tr> \n",
    "<td>\n",
    "**cost**\n",
    "</td>\n",
    "<td>\n",
    "0.27782375\n",
    "</td>\n",
    "</tr> \n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Calculer les gradients automatiquement (Automatic differentiation) \n",
    "\n",
    "Le calcul des gradients ([Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) ) est une des plus importantes parties de Tensorflow car elle permet l'apprentissage des réseaux de neurones par [rétropropagation](https://en.wikipedia.org/wiki/Backpropagation).\n",
    "On utilisera le gestionnaire de contexte [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape?version=stable) pour calculer les gradients. \n",
    "\n",
    "L'exemple suivant montre comment calculer les gradients pour la fonction $ y = x^2$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradient computation with GradientTape ###\n",
    "\n",
    "# y = x^2\n",
    "# Example: x = 3.0\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "# Initiate the gradient tape\n",
    "with tf.GradientTape() as tape:\n",
    "  # Define the function\n",
    "  y = x * x\n",
    "    \n",
    "# Access the gradient -- derivative of y with respect to x\n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "assert dy_dx.numpy() == 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de l'exemple précédent utilisant `GradientTape`, nous allons examiner un exemple où nous utilisons la différenciation automatique et la SGD pour trouver le minimum de $J=(x-x_f)^2$. Ici, $x_f$ est une variable pour une valeur souhaitée que nous essayons d'estimer ; $J$ représente un coût que nous essayons de minimiser. \n",
    "\n",
    "Bien que nous puissions clairement résoudre ce problème de manière analytique ($x_{min}=x_f$), on va le faire avec `GradientTape`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing x=[[1.6940167]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'x value')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Function minimization with automatic differentiation and SGD ###\n",
    "\n",
    "# Initialize a random value for our initial x\n",
    "x = tf.Variable([tf.random.normal([1])])\n",
    "print(\"Initializing x={}\".format(x.numpy()))\n",
    "\n",
    "learning_rate = 1e-2 # learning rate for SGD\n",
    "history = []\n",
    "# Define the target value\n",
    "x_f = 4\n",
    "\n",
    "# We will run SGD for a number of iterations. At each iteration, we compute the loss, \n",
    "#   compute the derivative of the loss with respect to x, and perform the SGD update.\n",
    "for i in range(500):\n",
    "    with tf.GradientTape() as tape:\n",
    "        ''' START CODE HERE '''\n",
    "        ''' define the loss as described above '''\n",
    "        loss =  tf.math.squared_difference(x, x_f)# TODO\n",
    "        ''' END CODE HERE '''\n",
    "    \n",
    "    # loss minimization using gradient tape\n",
    "    grad = tape.gradient(loss, x) # compute the derivative of the loss with respect to x\n",
    "    new_x = x - learning_rate*grad # sgd update\n",
    "    x.assign(new_x) # update the value of x\n",
    "    history.append(x.numpy()[0])\n",
    "\n",
    "# Plot the evolution of x as we optimize towards x_f!\n",
    "plt.plot(history)\n",
    "plt.plot([0, 500],[x_f,x_f])\n",
    "plt.legend(('Predicted', 'True'))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('x value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can compute higher-order derivatives by nesting tapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.5934568  0.7498028 ]\n",
      " [0.05557543 0.00696731]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "a = tf.Variable(a)\n",
    "\n",
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as tape:\n",
    "        c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "        dc_da = tape.gradient(c, a)\n",
    "    d2c_da2 = outer_tape.gradient(dc_da, a)\n",
    "    print(d2c_da2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 - Conversion de labels: one-hot-encoding\n",
    "\n",
    "Souvent, en machine learning, vous aurez un vecteur $y$ avec des nombres allant de $0$ à $C$, où $C$ est le nombre de classes. Si $C$ est par exemple 4, alors vous pourriez avoir le vecteur $y$ suivant que vous devrez convertir comme suit :\n",
    "\n",
    "\n",
    "<img src=\"imgs/onehot.png\" style=\"width:600px;height:150px ;\">\n",
    "\n",
    "On appelle cela un encodage \"one hot\", car dans la représentation finale, un seul élément de chaque colonne est \"hot\" (c'est-à-dire mis à 1). Dans tensorflow, vous pouvez utiliser une ligne de code : \n",
    "\n",
    "- tf.one_hot(labels, class_nb, axis) \n",
    "\n",
    "**Exercice : ** Implémentez la fonction ci-dessous pour prendre un vecteur d'étiquettes et le nombre total de classes $C$, et encoder le en one-hot. Utilisez `tf.one_hot()` pour faire cela. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_hot_matrix\n",
    "\n",
    "def one_hot_matrix(labels, class_nb):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    class_nb -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ''' START CODE HERE '''\n",
    "   \n",
    "    print(class_nb)\n",
    "    # Create a tf.constant equal to class_nb (depth), name it 'class_nb'. (approx. 1 line)\n",
    "    class_nb = tf.constant(class_nb)\n",
    "    print(class_nb)\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(labels, class_nb) \n",
    " \n",
    "    ''' END CODE HERE '''\n",
    "    \n",
    "    return one_hot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "one_hot = tf.Tensor(\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]], shape=(6, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1,2,3,0,2,1])\n",
    "one_hot = one_hot_matrix(labels, class_nb = 4)\n",
    "print (\"one_hot = \" + str(one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "    <tr> \n",
    "        <td>\n",
    "            **one_hot**\n",
    "        </td>\n",
    "        <td>\n",
    "        [[ 0.  0.  0.  1.  0.  0.]\n",
    " [ 1.  0.  0.  0.  0.  1.]\n",
    " [ 0.  1.  0.  0.  1.  0.]\n",
    " [ 0.  0.  1.  0.  0.  0.]]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
